<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gouthamsaipydi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gouthamsaipydi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-22T17:35:57+00:00</updated><id>https://gouthamsaipydi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">7 Pandas Methods everybody should know</title><link href="https://gouthamsaipydi.github.io/blog/2025/7-Pandas-Methods-everybody-should-know/" rel="alternate" type="text/html" title="7 Pandas Methods everybody should know"/><published>2025-09-22T00:00:00+00:00</published><updated>2025-09-22T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/7%20Pandas%20Methods%20everybody%20should%20know</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/7-Pandas-Methods-everybody-should-know/"><![CDATA[<p>Lo and Behold, here i am back with the 7 most important methods in pandas that every data scientist should be aware of.</p> <p>Note: I’ll be skipping over the basic methods like <code class="language-plaintext highlighter-rouge">head()</code> and <code class="language-plaintext highlighter-rouge">info()</code>, coz they are the best.</p> <p>1) <code class="language-plaintext highlighter-rouge">describe()</code> - This method can be applied on both Data Frames and Series; its a quick way to breeze through the descriptive statistics of the columns involved in the dataset. - The 5 point summary of data can be a good starting point for getting an idea about the data distribution. The Five Numbers are Minimum, Q1(1st Quartile), Median, Q3(3rd Quartile) and Maximum - Along with that, the method also provides the frequency(count),mean and standard deviation to better understand the spread. - For object(string) columns; the above method provides us with the count, number of unique objects, the most frequent element and also its frequency.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```python
import pandas as pd 
df=pd.DataFrame({'Age':[14,35,25,36,19],'Height':[144,157,178,181,171]})
df.describe()
#output
#&gt;&gt;&gt; df.describe()
#         Age      Height
#count   5.000000    5.000000
#mean   25.800000  166.200000
#std     9.679876   15.482248
#min    14.000000  144.000000
#25%    19.000000  157.000000
#50%    25.000000  171.000000
#75%    35.000000  178.000000
#max    36.000000  181.000000 
``` 2) `loc` &amp; `iloc`
Both these methods are an absolute must-knows. They are used to retrieve subsets of the data. 
loc :- label based indexing
iloc:- integer based indexing

 An example would clarify the difference between them well
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">:[</span><span class="mi">14</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">36</span><span class="p">,</span><span class="mi">19</span><span class="p">],</span><span class="sh">'</span><span class="s">Height</span><span class="sh">'</span><span class="p">:[</span><span class="mi">144</span><span class="p">,</span><span class="mi">157</span><span class="p">,</span><span class="mi">178</span><span class="p">,</span><span class="mi">181</span><span class="p">,</span><span class="mi">171</span><span class="p">]})</span>
	
	<span class="c1"># Let me write the code to extract the first column using both the methods 
</span>	<span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>  
	<span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">]</span> 
</code></pre></div></div> <p>3) <code class="language-plaintext highlighter-rouge">groupby</code> Its the go-to method for any data aggregation task, if you want to understand the inter-relationship between features. ```python #Lets up the ante by making the dataframe abit more diverse df=pd.DataFrame({‘Age’:[14,35,25,36,19],’Height’:[144,157,178,181,171],’Gender’:[‘Male’,’Female’,’Female’,’Male’,’Female’]})</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Suppose we are interested in the average height and age for both males and female, fret not my friend, Here comes groupby to the rescue. 
df.groupby(by='Gender')['Height'].mean() # average height of both the classes
# similarly 
df.groupby(by='Gender')['Age'].mean() # average age of both the classes ```
</code></pre></div></div> <p>4) <code class="language-plaintext highlighter-rouge">value_counts</code> If you are working with a categorical feature and would want to find the number of occurrences of each class, then <code class="language-plaintext highlighter-rouge">value_counts</code> is the method you should be using. Let me run you through a quick example, ```python df=pd.DataFrame({‘Age’:[14,35,25,36,19],’Height’:[144,157,178,181,171],’Gender’:[‘Male’,’Female’,’Female’,’Male’,’Female’]})</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#to find the number males and females in the data, we could use 
df['Gender'].value_counts() ```
</code></pre></div></div> <p>5) <code class="language-plaintext highlighter-rouge">pivot_table</code> - It’s the same pivot table which used to give us nightmares in Excel. But Pandas makes it a tad bit easy is what i believe. - With pivot table, you can build super complex aggregations, sometimes the code just bends my mind. - Here is the syntax for the curious few of you: <code class="language-plaintext highlighter-rouge">pandas.pivot_table(_data_, _values=None_, _index=None_, _columns=None_, _aggfunc='mean'_, _fill_value=None_, _margins=False_, _dropna=True_, _margins_name='All'_, _observed=&lt;no_default&gt;_, _sort=True_)</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```python
df=pd.DataFrame({'Age':[14,35,25,36,19],'Height':[144,157,178,181,171],'Gender':['Male','Female','Female','Male','Female']})

pd.pivot_table(df,values=['Age','Height'],index='Gender',aggfunc='mean')
#Output
#Gender
#Female  26.333333  168.666667
#Male    25.000000  162.500000
```  6) `apply`
-  If you wanna apply a function to all the entries of a column, then apply will make your life simple. 
- Just define the operation as a regular function and then just *apply* it.
- If you want to go more pythonic, then try lambda functions(they just make the code look more sleek)
```python
def lower(s: str)-&gt; str:
	return s.lower()

df=pd.DataFrame({'Age':[14,35,25,36,19],'Height':[144,157,178,181,171],'Gender':['Male','Female','Female','Male','Female']})

df['Gender']=df['Gender'].apply(lower)
``` 7) `replace`
- The name of the method simply explains what it does, replaces the values you want to be replaced with the given replacement. 
```python
df=pd.DataFrame({'Age':[24,35,25,36,29],'Height':[144,157,178,181,171],'Gender':['Male','Female','Female','Male','Female'],'Marital Status':['Not Interested','Alone','Married','Depressed','Single']})

df.replace(to_replace=['Not Interested','Alone','Depressed'],value='Single')
```
</code></pre></div></div> <p>Pandas is a great library and has plenty more mind boggling methods which can make our lives simpler.</p> <p>Head over to the official <a href="https://pandas.pydata.org/docs/index.html">Pandas</a>documentation to read more about them.</p> <p>Happy Reading!</p>]]></content><author><name></name></author><category term="Pandas"/><summary type="html"><![CDATA[A short compilation of the most used pandas methods]]></summary></entry><entry><title type="html">7 Pandas Methods everybody should know</title><link href="https://gouthamsaipydi.github.io/blog/2025/7_Pandas_Methods_everybody_should_know/" rel="alternate" type="text/html" title="7 Pandas Methods everybody should know"/><published>2025-09-21T00:00:00+00:00</published><updated>2025-09-21T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/7_Pandas_Methods_everybody_should_know</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/7_Pandas_Methods_everybody_should_know/"><![CDATA[<p>Lo and Behold, here i am back with the 7 most important methods in pandas that every data scientist should be aware of.</p> <p>Note: I’ll be skipping over the basic methods like <code class="language-plaintext highlighter-rouge">head()</code> and <code class="language-plaintext highlighter-rouge">info()</code>, coz they are the best.</p> <p>1) <code class="language-plaintext highlighter-rouge">describe()</code> - This method can be applied on both Data Frames and Series; its a quick way to breeze through the descriptive statistics of the columns involved in the dataset. - The 5 point summary of data can be a good starting point for getting an idea about the data distribution. The Five Numbers are Minimum, Q1(1st Quartile), Median, Q3(3rd Quartile) and Maximum - Along with that, the method also provides the frequency(count),mean and standard deviation to better understand the spread. - For object(string) columns; the above method provides us with the count, number of unique objects, the most frequent element and also its frequency.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```python
import pandas as pd 
df=pd.DataFrame({'Age':[14,35,25,36,19],'Height':[144,157,178,181,171]})
df.describe()
#output
#&gt;&gt;&gt; df.describe()
#         Age      Height
#count   5.000000    5.000000
#mean   25.800000  166.200000
#std     9.679876   15.482248
#min    14.000000  144.000000
#25%    19.000000  157.000000
#50%    25.000000  171.000000
#75%    35.000000  178.000000
#max    36.000000  181.000000 
``` 2) `loc` &amp; `iloc`
Both these methods are an absolute must-knows. They are used to retrieve subsets of the data. 
loc :- label based indexing
iloc:- integer based indexing

 An example would clarify the difference between them well
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">:[</span><span class="mi">14</span><span class="p">,</span><span class="mi">35</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mi">36</span><span class="p">,</span><span class="mi">19</span><span class="p">],</span><span class="sh">'</span><span class="s">Height</span><span class="sh">'</span><span class="p">:[</span><span class="mi">144</span><span class="p">,</span><span class="mi">157</span><span class="p">,</span><span class="mi">178</span><span class="p">,</span><span class="mi">181</span><span class="p">,</span><span class="mi">171</span><span class="p">]})</span>
	
	<span class="c1"># Let me write the code to extract the first column using both the methods 
</span>	<span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>  
	<span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">]</span> 
</code></pre></div></div> <p>3) <code class="language-plaintext highlighter-rouge">groupby</code> Its the go-to method for any data aggregation task, if you want to understand the inter-relationship between features. ```python #Lets up the ante by making the dataframe abit more diverse df=pd.DataFrame({‘Age’:[14,35,25,36,19],’Height’:[144,157,178,181,171],’Gender’:[‘Male’,’Female’,’Female’,’Male’,’Female’]})</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Suppose we are interested in the average height and age for both males and female, fret not my friend, Here comes groupby to the rescue. 
df.groupby(by='Gender')['Height'].mean() # average height of both the classes
# similarly 
df.groupby(by='Gender')['Age'].mean() # average age of both the classes ```
</code></pre></div></div> <p>4) <code class="language-plaintext highlighter-rouge">value_counts</code> If you are working with a categorical feature and would want to find the number of occurrences of each class, then <code class="language-plaintext highlighter-rouge">value_counts</code> is the method you should be using. Let me run you through a quick example, ```python df=pd.DataFrame({‘Age’:[14,35,25,36,19],’Height’:[144,157,178,181,171],’Gender’:[‘Male’,’Female’,’Female’,’Male’,’Female’]})</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#to find the number males and females in the data, we could use 
df['Gender'].value_counts() ```
</code></pre></div></div> <p>5) <code class="language-plaintext highlighter-rouge">pivot_table</code> - It’s the same pivot table which used to give us nightmares in Excel. But Pandas makes it a tad bit easy is what i believe. - With pivot table, you can build super complex aggregations, sometimes the code just bends my mind. - Here is the syntax for the curious few of you: <code class="language-plaintext highlighter-rouge">pandas.pivot_table(_data_, _values=None_, _index=None_, _columns=None_, _aggfunc='mean'_, _fill_value=None_, _margins=False_, _dropna=True_, _margins_name='All'_, _observed=&lt;no_default&gt;_, _sort=True_)</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```python
df=pd.DataFrame({'Age':[14,35,25,36,19],'Height':[144,157,178,181,171],'Gender':['Male','Female','Female','Male','Female']})

pd.pivot_table(df,values=['Age','Height'],index='Gender',aggfunc='mean')
#Output
#Gender
#Female  26.333333  168.666667
#Male    25.000000  162.500000
```  6) `apply`
-  If you wanna apply a function to all the entries of a column, then apply will make your life simple. 
- Just define the operation as a regular function and then just *apply* it.
- If you want to go more pythonic, then try lambda functions(they just make the code look more sleek)
```python
def lower(s: str)-&gt; str:
	return s.lower()

df=pd.DataFrame({'Age':[14,35,25,36,19],'Height':[144,157,178,181,171],'Gender':['Male','Female','Female','Male','Female']})

df['Gender']=df['Gender'].apply(lower)
``` 7) `replace`
- The name of the method simply explains what it does, replaces the values you want to be replaced with the given replacement. 
```python
df=pd.DataFrame({'Age':[24,35,25,36,29],'Height':[144,157,178,181,171],'Gender':['Male','Female','Female','Male','Female'],'Marital Status':['Not Interested','Alone','Married','Depressed','Single']})

df.replace(to_replace=['Not Interested','Alone','Depressed'],value='Single')
```
</code></pre></div></div> <p>Pandas is a great library and has plenty more mind boggling methods which can make our lives simpler.</p> <p>Head over to the official <a href="https://pandas.pydata.org/docs/index.html">Pandas</a>documentation to read more about them.</p> <p>Happy Reading!</p>]]></content><author><name></name></author><category term="Pandas"/><summary type="html"><![CDATA[A short compilation of the most used pandas methods]]></summary></entry><entry><title type="html">Gradient Descent from Scratch</title><link href="https://gouthamsaipydi.github.io/blog/2025/Gradient_Descent/" rel="alternate" type="text/html" title="Gradient Descent from Scratch"/><published>2025-07-31T00:00:00+00:00</published><updated>2025-07-31T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/Gradient_Descent</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/Gradient_Descent/"><![CDATA[<p>Hell yeah, Here we go again! It’s been some time, that’s for sure.</p> <p>I’m thinking of starting a new series of blog posts where we’ll be trying to implement things from scratch; Let’s kick start things with gradient descent algorithm(from scratch) with only using NumPy(Numerical Python Library). I’ll be writing the code alongside the theory to better understand this super awesome algorithm.</p> <p>There are plenty of guides over the internet on this, you can name it protege effect or anything, but teaching others only enhances your own clarity on the topic. Okay lets cut to the chase and jump right into the topic.</p> <h2 id="introduction">Introduction</h2> <p>With the onset of big data- Neural nets have found there way into the mainstream. To train these mammoth neural nets, we are always looking for ways to save our expenses while not compromising on the quality of the training.</p> <h3 id="whats-our-approach">What’s our approach?</h3> <p>We are trying to minimize a “decided” Loss function(with loads of parameters, like loads and loads), i.e., looking out for suitable values for those parameters which minimize the above decided loss function.</p> <p>Note: I’m using <em>decided</em>, as the loss depends on the problem we are trying to solve.</p> <p>More mathematically, we have an optimization problem as, \(\min_{w\in\mathbb{R}^D}E(w)\) \(w^*=\text{arg} \min_{w\in\mathbb{R}^D}E(w)\)</p> <p>$w^*$ is the vector, why we are doing all this. It will be our gateway to awesome predictions.</p> <p>The Objective function [$E(w)$ above] is usually a super complicated function; by complicated, i mean some function which cannot be solved in closed form($w^*$= some function involving the independent variables), and needs some deep introspection to find its global minima.</p> <p>There are plenty of iterative methods to solve for the above optimisation problems. Like the Newton Method and all, but the one which has took the world by storm is- Gradient Descent and Stochastic Gradient descent, a clever extension which serves as the basis for training super large neural nets.</p> <h2 id="gradient-descent">Gradient Descent</h2> <p>It’s basically an Algorithm, if followed with the right-set of hyperparamters will fetch you the golden $w^*$ that you are looking out for.</p> <p>Getting into math of it, its an iterative first-order algorithm for minimizing a differentiable multivariate function.</p> <p>Let’s get a few mechanical details out of the way;</p> <p>Gradient Descent $\iff$ used for finding the local minima of multivariate functions</p> <p>Gradient Ascent $\iff$ used for finding the local maxima of multivariate functions</p> <p>Let me walk you through a short lecture on gradients,</p> <table> <tbody> <tr> <td>For a Multi-Variable function $F(x)$ defined and differentiable in a neighborhood of a point $a$, then $F(x)$ increases the fastest if one goes from $a$ in the direction of the gradient of $F$ at $a$ i.e., $\frac{\partial F}{\partial x}{\Large</td> <td>}_{x=a}$</td> </tr> </tbody> </table> <p>In simple English, a recursive operation to look for the local minimum of seemingly daunting functions.</p> <p>Idea is that we use iterative methods to solve the optimization problem, \(w^{(0)}\rightarrow w^{(1)}... \rightarrow w^{(\infty)}=w^\star\)</p> <p>We’ll be initializing the weight vector $w^{(0)}$ usually with small random numbers; I’m using the zero index to signify that as our initial guess of optimal parameters. We’ll continue using our recursive function to get hold of the minimum.</p> <h3 id="when-to-stop">When to stop?</h3> <p>This is a million-dollar question and the stopping criteria can vary- It can be limited to a set number of iterations. The difference between the updates is quite small and many others.</p> <p>Which one to choose is simply upto the user and the problem that their trying to solve.</p> <p>Let’s jump back to the algorithm,</p> <p><em>Notation Alert: The number of training samples ${(x^{(i)},y^{(i)})}$ is $m$</em></p> <blockquote> <p><strong>Example</strong> Eg: $w_d^{(0)}\sim$ uniform$[-0.01,0.01]$ : $\forall d$ Initial weight vector $w^{(0)}$: usually small random numbers.</p> <p>For Demonstration purpose, lets assume the squared loss function (the el classico of all loss functions), and later I will work out the details for $L^1$ Loss function as well.</p> <ul> <li>We are working with a supervised learning algorithm, so there will be a training set of the form,<br/>    $T={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$<br/>    where each $x^{(i)}$ is a $p$ dimensional vector $: \forall i$</li> <li>Consider the ordinary least square regression model, then the hypothesis function is: $h_{\theta}(x)=\sum_{i=0}^{d}\theta_i x_i=\theta^Tx$ where $x_0=1$</li> <li>The $L^2$ Loss function is  $\mathbb{L}(y_i,\hat{y_i})=(y_i-\hat{y_i})^2$</li> <li>if we take the sum of all the loss functions over the training set, we get the cost function</li> <li>$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}L(y_i,\hat{y_i})=\frac{1}{2}\sum_{i=1}^{m} L(y_i,h_{\theta}(x_i))=\frac{1}{2}\sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2$</li> </ul> <p>Now this is Cost Function, we are looking for the values which minimize this function.</p> </blockquote> <p>Detail to ponder over: Here the half($\frac{1}{2}$) in cost function is added to simplify the calculation and nothing else. Any positive monotonic transformation of the objective function won’t change the points of minima($arg \hspace{2 mm}min$).</p> <blockquote> <p>Gradient Descent Algorithm</p> <ul> <li>Update rule $\theta_j \leftarrow \theta_j +\Delta \theta_j$ with an update $\Delta \theta_j =-\alpha \frac{\partial J(\theta)}{\partial \theta_j}$, element wise for $d=1,…,m$</li> </ul> <p>for the above $L^2$ Loss function, upon working out the derivative, &gt;we get it as the following, \(\frac{\partial J(\theta)}{\partial \theta}= \sum_{i=1}^{m} (\theta^T x^{(i)}-y^{(i)})x^{(i)}\)</p> <p>Therefore, the update will become, \(\theta:= \theta + \alpha \sum_{i=1}^m (y^{i}-\theta^T x^{i})x^{i}\)</p> <p>To sum it up in vector notation, it will be(for a generic cost &gt;function $J(\theta)$) \(\theta :=\theta -\alpha \nabla_\theta J(\theta)\)</p> </blockquote> <p>Here is the code implementation of it, I’ll briefly discuss the idea of it, so basically we are building a class which will take in the training data and</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <p>Thats all the library imports we’ll be using</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">training_data</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        training_data: The data that is to be considered for the supervised learning problem </span><span class="se">\n</span><span class="s">
        start: Initial guess of the parameters </span><span class="se">\n</span><span class="s">
        learning_rate: Rate which determines the step size for every iteration </span><span class="se">\n</span><span class="s">
        num_iterations: Total Number of Iterations
        </span><span class="sh">"""</span>
        <span class="c1"># Convert training data to matrices
</span>        <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x_vec</span><span class="p">,</span> <span class="n">y_val</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
            <span class="n">X</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="n">x_vec</span><span class="p">))</span>  <span class="c1"># Add bias term
</span>            <span class="n">y</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Shape: (m, n+1) where m=samples, n=features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Shape: (m,)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span> <span class="c1"># number of training examples
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">start</span><span class="p">)</span> <span class="c1"># Inital starting point
</span>        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>
        <span class="n">self</span><span class="p">.</span><span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tuple</span><span class="p">(</span><span class="n">start</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">compute_gradient_loop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">current</span><span class="p">):</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
            <span class="n">x_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># i-th training example
</span>            <span class="n">y_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># i-th target
</span>            <span class="n">residual</span> <span class="o">=</span> <span class="n">y_i</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">x_i</span><span class="p">)</span>
            <span class="n">grad</span> <span class="o">+=</span> <span class="n">residual</span> <span class="o">*</span> <span class="n">x_i</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">current</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">):</span>
            <span class="n">grad</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_gradient_loop</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">current</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
            <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">current</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">query_point</span><span class="p">):</span>
        <span class="n">query_point_with_dummy</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">query_point</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">query_point_with_dummy</span><span class="p">))</span>

</code></pre></div></div> <h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2> <p>Now lets build another class which will run the stochastic gradient descent, as the name suggest, it is stochastic- the parameter value update is now based on the gradient of a single training example(choosen randomly from the training set) and this greatly reduces the run-time(as now you wont be using the all the training samples to calculate the gradient).</p> <p>I’ll show the results. Particularly when the training set is large, the Stochastic Gradient Descent is preferred over the Batch Gradient Descent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">class</span> <span class="nc">StochasticGradientDescent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">training_data</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        training_data: The data that is to be considered for the supervised learning problem </span><span class="se">\n</span><span class="s">
        start: Initial guess of the parameters </span><span class="se">\n</span><span class="s">
        learning_rate: Rate which determines the step size for every iteration </span><span class="se">\n</span><span class="s">
        num_iterations: Total Number of Iterations
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="o">=</span><span class="p">[(</span><span class="nf">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="o">=</span><span class="n">start</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="o">=</span><span class="n">num_iterations</span>
        <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="o">=</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">point</span><span class="p">,</span><span class="n">i</span><span class="p">):</span>
        <span class="n">grad</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
        <span class="n">point</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
        <span class="n">x_i</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y_i</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">residual</span><span class="o">=</span><span class="n">y_i</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">point</span><span class="p">,</span><span class="n">x_i</span><span class="p">)</span>
        <span class="n">grad</span><span class="o">+=</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span><span class="o">*</span><span class="n">x_i</span>
        <span class="k">return</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">):</span>
            <span class="n">i</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">))</span>
            <span class="n">current</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">gradient</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_gradient</span><span class="p">(</span><span class="n">current</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
            <span class="n">update</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">current</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">current</span><span class="p">)))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">query_point</span><span class="p">):</span>
        <span class="n">query_point_with_dummy</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">query_point</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">query_point_with_dummy</span><span class="p">))</span>
</code></pre></div></div> <p>Now let’s put these two algorithms to a race,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#importing the time module to compare the run-time
</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="p">[([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="mf">12.0</span><span class="p">),([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">6.0</span><span class="p">],</span> <span class="mf">19.0</span><span class="p">),</span> <span class="p">([</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="mf">15.0</span><span class="p">)]</span>

<span class="n">gd</span> <span class="o">=</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">10e5</span><span class="p">),</span> <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">)</span>
<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">gd</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">parameters from Batch GD: </span><span class="si">{</span><span class="n">gd</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">time taken: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">prediction for (3,3,3,3): </span><span class="si">{</span><span class="n">gd</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="n">sgd</span> <span class="o">=</span> <span class="nc">StochasticGradientDescent</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">10e5</span><span class="p">),</span> <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">)</span>
<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">sgd</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">parameters from Stochastic GD: </span><span class="si">{</span><span class="n">sgd</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">time taken: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">prediction for (3,3,3,3): </span><span class="si">{</span><span class="n">sgd</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Let’s take a look at the output,</p> <p><em>Red Alert: I have a pretty normal Laptop</em></p> <pre><code class="language-output">parameters from Batch GD: (0.2341891045710697, 1.0012523481527535, 0.9993738259236024, 0.9367564182842788, 1.003757044458375)

time taken: 28.767800092697144

prediction for (3,3,3,3): 12.057608015028098

parameters from Stochastic GD: (0.23418910457018247, 1.001252348153782, 0.9993738259231028, 0.9367564182807299, 1.003757044461383)

time taken: 25.743896484375

prediction for (3,3,3,3): 12.057608015027176
</code></pre> <p>We can see that for a mere $1,00,000$ iterations, we get the parameter estimates from both the algorithms to be very very close. Another point worth noting is the fact that, the Stochastic Gradient Descent was faster than Gradient Decent by more than 4 seconds on my laptop. This could be exploited to a great extent when the training set is huge.</p> <p>Literature also has the mention of Stochastic Batch Gradient Descent(SBGD), which is a slight ugrade over the vanilla SGD as here in SBGD, we sample n(Batch Size) without replacement and go about following the same steps of SGD. SBGD is proven to yield us better results but just a tad bit noiser in the training.</p> <hr/> <h3 id="conclusion">Conclusion</h3> <p>In summary, these two algorithms are commonly used to training large neural nets(or, some convex combination of them). The underlying mathematical foundations underlying these methods are truly elegant!</p> <p>If you are interested in experimenting with the code, you can access the accompanying Python notebook for hands-on practice.</p> <p>Happy Learning!</p>]]></content><author><name></name></author><category term="From Scratch"/><summary type="html"><![CDATA[Hell yeah, Here we go again! It’s been some time, that’s for sure.]]></summary></entry><entry><title type="html">Stack and Queue Implementation</title><link href="https://gouthamsaipydi.github.io/blog/2025/Stack_and_Queue/" rel="alternate" type="text/html" title="Stack and Queue Implementation"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/Stack_and_Queue</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/Stack_and_Queue/"><![CDATA[<p>Here I plan to document the Stack and Queue Implementations in Python, these could be helpful to anyone preparing for the upcoming placement season.</p> <h2 id="stack">Stack</h2> <p>Simply put it’s a linear data structure(linear data container) which holds data with a few quirky twist: 1) Data can only be inserted from one end 2) Inserted data can be removed from that very en</p> <p>I know I made it a bit wordy, the famous 4 letter acronymn LIFO(Last-in First-out) principle nicely sums up the above jargon.</p> <p>Now Let’s implement it in Python to see its beauty.</p> <pre><code class="language-Python">class stack:
    def __init__(self,size):
        if size&lt;=0 or type(size)==float:
            raise ValueError('Stack size must be a positive integer')
        self.size=size
        self.top=-1
        self.arr=[]

    def push(self,x):
        if self.top&gt;=self.size-1:
            raise OverflowError('Stack is full')
        self.arr.append(x)
        self.top+=1
    
    def pop(self):
        if self.top==-1:
            raise IndexError('The Stack is empty, cannot pop')
        self.top-=1
        self.arr.pop()


    def peek(self):
        if self.top==-1:
            raise IndexError('The Stack is empty')
        return self.arr[self.top]

    def length(self):
        return self.top+1
</code></pre> <h3 id="the-important-methods-involved-with-stack-data-structure-are">The Important Methods involved with Stack Data Structure are</h3> <p>push, pop, peek and length.</p> <p>Push Method: As the word suggests, it pushes elements on to the stack from one end.</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Pop Method: It pops the last element which was pushed onto the stack(LIFO is the Magic Mantra)</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Peek Method: This method enables us to take a quick peek at the stack.</p> <ul> <li>Time Complexity involed is O(1)</li> </ul> <p>Length Method: This method enables us to get the size of the stack.</p> <h2 id="queue">Queue</h2> <p>It’s also a linear data structure which holds data in the following way: 1) Data can only be inserted from one end 2) Inserted data can be removed from the opposite end</p> <p>The 4 letter acronymn which can come in handy is FIFO(First-in First-out)</p> <p>Now Let’s implement it in Python to better understand this data structure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Queue</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">size</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span><span class="o">*</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nc">OverflowError</span><span class="p">(</span><span class="sh">"</span><span class="s">Queue is full</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="n">self</span><span class="p">.</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="p">]</span><span class="o">=</span><span class="n">x</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">+=</span><span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">IndexError</span><span class="p">(</span><span class="sh">"</span><span class="s">Queue is empty</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">a</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="n">self</span><span class="p">.</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">-=</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">peek</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nc">IndexError</span><span class="p">(</span><span class="sh">"</span><span class="s">Queue is Empty</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">length</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span>
</code></pre></div></div> <h3 id="the-important-methods-involved-with-queue-data-structure-are">The Important Methods involved with Queue Data Structure are</h3> <p>push, pop, peek and length.</p> <p>Push Method: It pushes elements on to the queue from one end.</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Pop Method: It pops the last element which was pushed first onto the queue</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Peek Method: This method enables us to take a quick peek at the queue</p> <ul> <li>Time Complexity involed is O(1)</li> </ul> <p>Length Method: This method enables us to get the size of the queue.</p> <ul> <li>Time Complexity involed is O(1)</li> </ul>]]></content><author><name></name></author><category term="DSA"/><category term="python"/><summary type="html"><![CDATA[Here I plan to document the Stack and Queue Implementations in Python, these could be helpful to anyone preparing for the upcoming placement season.]]></summary></entry><entry><title type="html">Arguably the Most Handy Python Library- Sympy</title><link href="https://gouthamsaipydi.github.io/blog/2025/Sympy/" rel="alternate" type="text/html" title="Arguably the Most Handy Python Library- Sympy"/><published>2025-01-20T00:00:00+00:00</published><updated>2025-01-20T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/Sympy</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/Sympy/"><![CDATA[<h2 id="sympy--the-library-hiding-under-the-hood">Sympy- The Library hiding under the hood</h2> <p>For the longest time, I had no clue this library was out there, and when I finally found it,It just blew my mind, I couldn’t stop thinking, “Why didn’t I know about this sooner?” But its just no use crying over spilled milk. Now let’s get straight to the good stuff and explore the magic of Sympy.</p> <p><a href="https://www.sympy.org/en/index.html">Sympy</a> stands for Symbolic Python. It was developed 17 years ago. Be it ODE’s or algebraic equations or solving the diophantine equations algebraically, look no further-Sympy has got you covered.</p> <h3 id="whats-so-special-about-this">What’s so special about this?</h3> <p>As most of you might be aware of the python library <a href="https://numpy.org/">Numpy</a>, which enables you to perform numerical calculations on your data. But Sympy is a bit more abstract in that sense, it enables you to perform symbolic computation(Using symbols)</p> <p>Simply put, we can perform all those nightmare inducing, pain staking and laborious mathematical operations like solving ODEs,substitution, differentiation, integration and even writing the equation in LaTex using sympy.</p> <p>Let me show you the beauty of it. It goes without saying that to use the library you need to first pip it :-}</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sympy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">f,x</span><span class="sh">'</span><span class="p">)</span>

<span class="c1">#Lets get creative about the function 
</span><span class="n">f</span><span class="o">=</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>

<span class="c1">#Interested in the functional value of the above function at x=2? Fret not dear
</span><span class="n">f</span><span class="p">.</span><span class="nf">subs</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)])</span>
<span class="c1"># Output 54.5981500331442
</span>
<span class="c1"># Lets differentiate this Monster 
</span><span class="n">f</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># 1.5*x**2*exp(x**3)**0.5
</span></code></pre></div></div> <p>That’s just a short premise into how useful this library can be.</p> <p>Here is another snippet which performs the following daunting mathematical operation easily</p> \[\int_0^{3\pi}x^{12}*sin(33x)\hspace{1mm}dx\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">f</span><span class="sh">'</span><span class="p">,</span><span class="n">cls</span><span class="o">=</span><span class="n">Function</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">)</span>
<span class="c1">#defining the function
</span><span class="n">f</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">12</span><span class="o">*</span><span class="nf">sin</span><span class="p">(</span><span class="mi">33</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="n">f</span><span class="p">.</span><span class="nf">integrate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1">#Output: -x**12*cos(33*x)/33 + 4*x**11*sin(33*x)/363 + 4*x**10*cos(33*x)/1089 - 40*x**9*sin(33*x)/35937 - 40*x**8*cos(33*x)/131769 + 320*x**7*sin(33*x)/4348377 + 2240*x**6*cos(33*x)/143496441 - 4480*x**5*sin(33*x)/1578460851 - 22400*x**4*cos(33*x)/52089208083 + 89600*x**3*sin(33*x)/1718943866739 + 89600*x**2*cos(33*x)/18908382534129 - 179200*x*sin(33*x)/623976623626257 - 179200*cos(33*x)/20591228579666481 
</span>
<span class="n">integral</span><span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="nf">integrate</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">subs</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">pi</span><span class="p">)])</span><span class="o">-</span><span class="n">f</span><span class="p">.</span><span class="nf">integrate</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">subs</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span><span class="mi">0</span><span class="p">)]))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">integral</span><span class="p">)</span>
<span class="c1"># Output: 14864390294.783045
</span></code></pre></div></div> <p>Another cool method which i found to be very useful was print_latex, I’ll use that method trying to solve the following ODE.</p> \[12*y^{\prime \prime }(x)+3y^{\prime}(x)-6*y(x)=0\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span><span class="n">cls</span><span class="o">=</span><span class="n">Function</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">dsolve</span><span class="p">(</span><span class="mi">12</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">6</span><span class="o">*</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="c1"># Output:
# Eq(y(x), C1*exp(x*(-1 + sqrt(33))/8) + C2*exp(-x*(1 + sqrt(33))/8))
</span><span class="nf">checkodesol</span><span class="p">(</span><span class="mi">12</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">6</span><span class="o">*</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: (True, 0)
</span><span class="nf">print_latex</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: y{\left(x \right)} = C_{1} e^{\frac{x \left(-1 + \sqrt{33}\right)}{8}} + C_{2} e^{- \frac{x \left(1 + \sqrt{33}\right)}{8}}
</span></code></pre></div></div> <p>Wondering what the solution, Lo and Behold here it is, \(y{\left(x \right)} = C_{1} e^{\frac{x \left(-1 + \sqrt{33}\right)}{8}} + C_{2} e^{- \frac{x \left(1 + \sqrt{33}\right)}{8}}\)</p> <p>Well here i have only mentioned a few instances where sympy can come in handy. Gloss over the <a href="https://docs.sympy.org/latest/index.html">Documentation</a>, you will find many more interesting applications.</p> <p>Happy Learning!</p>]]></content><author><name></name></author><category term="python"/><summary type="html"><![CDATA[Sympy- The Library hiding under the hood]]></summary></entry><entry><title type="html">What If?</title><link href="https://gouthamsaipydi.github.io/blog/2024/First_Blog_Post/" rel="alternate" type="text/html" title="What If?"/><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2024/First_Blog_Post</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2024/First_Blog_Post/"><![CDATA[<p>Welcome to my first blog post!</p> <p>I have always wondered how would it feel to able to express yourself without a care in the world. It’s this overarching curiosity that’s driving me to pen this down. I’m planning of documenting my views on the books and articles that i go through.</p> <p>Let me be clear with something, 1) English is not my first language, and 2) I am an amateur writer, don’t expect Shakespearean from me.</p> <p>Brace yourselves to experience an opionated mind!</p> <p>Prost</p>]]></content><author><name></name></author><category term="Introduction"/><summary type="html"><![CDATA[Hello World]]></summary></entry><entry><title type="html">Homo Sapien- A Two legged menace</title><link href="https://gouthamsaipydi.github.io/blog/2024/Sapiens/" rel="alternate" type="text/html" title="Homo Sapien- A Two legged menace"/><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2024/Sapiens</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2024/Sapiens/"><![CDATA[<p>Recently i had the opportunity of getting to start Yuval Noah Harari’s classic Sapiens. It has been a great read so far, I am planning to update this post as and when i complete a section of the book. In this post i plan to dig deep into the book with an existential take of course.</p> <h2 id="rise-to-the-top">Rise to the Top</h2> <p>Our rise from being a bunch of unimportant weaklings on the planet to the top of the food chain has been quite a journey, Even the best of the modern day UFC fighters would get their skulls beaten to pulp by a wild chimpanzee in a matter of seconds. The story of such a frail being’s rise to the throne is the book Sapiens.</p> <p>The Author goes in detail about what could be the reasons of our ascendency and the downfall of other species of the Homo genus. Let me digress a bit and explain you about our long lost cousins, We had company up until circa 10,000 BCE when the last neaderthal passed away, Leaving us orphaned forever. The Planet Earth was home to around 7 (as of now) species of Homo genus, there were Homo erectus in the Indonesian archipelagos, Homo Neandethal in the Europa and our kind in jungles of Eastern Africa. What favoured us and helped us to beat the ever changing climatic conditions and reign supreme is quite debated among the scholars.</p> <p>Lets evalute the obvious reasons that come to us,</p> <ul> <li>The Size of the Brain: Yes it is definitely larger when compared with other species, but recent archialogical evidences show that the Neaderthal had much larger craniums, hence larger grey matter.</li> <li></li> </ul> <h2 id="part-time-gossip-mongers">Part-time Gossip Mongers</h2> <p>## We are so clouded by the Imagined Reality, that we at times seem to forget how small and insignificant we are in this cosmos.</p>]]></content><author><name></name></author><category term="Book"/><summary type="html"><![CDATA[Recently i had the opportunity of getting to start Yuval Noah Harari’s classic Sapiens. It has been a great read so far, I am planning to update this post as and when i complete a section of the book. In this post i plan to dig deep into the book with an existential take of course.]]></summary></entry></feed>