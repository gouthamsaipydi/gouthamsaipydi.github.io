<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gouthamsaipydi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gouthamsaipydi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-31T15:30:14+00:00</updated><id>https://gouthamsaipydi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Gradient Descent from Scratch</title><link href="https://gouthamsaipydi.github.io/blog/2025/Gradient_Descent/" rel="alternate" type="text/html" title="Gradient Descent from Scratch"/><published>2025-07-31T00:00:00+00:00</published><updated>2025-07-31T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/Gradient_Descent</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/Gradient_Descent/"><![CDATA[<p>Hell yeah, Here we go again! It’s been some time, that’s for sure.</p> <p>I’m thinking of starting a new series of blog posts where we’ll be trying to implement things from scratch; Let’s kick start things with gradient descent algorithm(from scratch) with only using NumPy(Numerical Python Library). I’ll be writing the code alongside the theory to better understand this super awesome algorithm.</p> <p>There are plenty of guides over the internet on this, you can name it protege effect or anything, but teaching others only enhances your own clarity on the topic. Okay lets cut to the chase and jump right into the topic.</p> <h2 id="introduction">Introduction</h2> <p>With the onset of big data- Neural nets have found there way into the mainstream. To train these mammoth neural nets, we are always looking for ways to save our expenses while not compromising on the quality of the training.</p> <h3 id="whats-our-approach">What’s our approach?</h3> <p>We are trying to minimize a “decided” Loss function(with loads of parameters, like loads and loads), i.e., looking out for suitable values for those parameters which minimize the above decided loss function.</p> <p>Note: I’m using <em>decided</em>, as the loss depends on the problem we are trying to solve.</p> <p>More mathematically, we have an optimization problem as, \(\min_{w\in\mathbb{R}^D}E(w)\) \(w^*=\text{arg} \min_{w\in\mathbb{R}^D}E(w)\)</p> <p>$w^*$ is the vector, why we are doing all this. It will be our gateway to awesome predictions.</p> <p>The Objective function [$E(w)$ above] is usually a super complicated function; by complicated, i mean some function which cannot be solved in closed form($w^*$= some function involving the independent variables), and needs some deep introspection to find its global minima.</p> <p>There are plenty of iterative methods to solve for the above optimisation problems. Like the Newton Method and all, but the one which has took the world by storm is- Gradient Descent and Stochastic Gradient descent, a clever extension which serves as the basis for training super large neural nets.</p> <h2 id="gradient-descent">Gradient Descent</h2> <p>It’s basically an Algorithm, if followed with the right-set of hyperparamters will fetch you the golden $w^*$ that you are looking out for.</p> <p>Getting into math of it, its an iterative first-order algorithm for minimizing a differentiable multivariate function.</p> <p>Let’s get a few mechanical details out of the way;</p> <p>Gradient Descent $\iff$ used for finding the local minima of multivariate functions</p> <p>Gradient Ascent $\iff$ used for finding the local maxima of multivariate functions</p> <p>Let me walk you through a short lecture on gradients,</p> <table> <tbody> <tr> <td>For a Multi-Variable function $F(x)$ defined and differentiable in a neighborhood of a point $a$, then $F(x)$ increases the fastest if one goes from $a$ in the direction of the gradient of $F$ at $a$ i.e., $\frac{\partial F}{\partial x}{\Large</td> <td>}_{x=a}$</td> </tr> </tbody> </table> <p>In simple English, a recursive operation to look for the local minimum of seemingly daunting functions.</p> <p>Idea is that we use iterative methods to solve the optimization problem, \(w^{(0)}\rightarrow w^{(1)}... \rightarrow w^{(\infty)}=w^\star\)</p> <p>We’ll be initializing the weight vector $w^{(0)}$ usually with small random numbers; I’m using the zero index to signify that as our initial guess of optimal parameters. We’ll continue using our recursive function to get hold of the minimum.</p> <h3 id="when-to-stop">When to stop?</h3> <p>This is a million-dollar question and the stopping criteria can vary- It can be limited to a set number of iterations. The difference between the updates is quite small and many others.</p> <p>Which one to choose is simply upto the user and the problem that their trying to solve.</p> <p>Let’s jump back to the algorithm,</p> <p><em>Notation Alert: The number of training samples ${(x^{(i)},y^{(i)})}$ is $m$</em></p> <blockquote> <p><strong>Example</strong> Eg: $w_d^{(0)}\sim$ uniform$[-0.01,0.01]$ : $\forall d$ Initial weight vector $w^{(0)}$: usually small random numbers.</p> <p>For Demonstration purpose, lets assume the squared loss function (the el classico of all loss functions), and later I will work out the details for $L^1$ Loss function as well.</p> <ul> <li>We are working with a supervised learning algorithm, so there will be a training set of the form,<br/>    $T={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$<br/>    where each $x^{(i)}$ is a $p$ dimensional vector $: \forall i$</li> <li>Consider the ordinary least square regression model, then the hypothesis function is: $h_{\theta}(x)=\sum_{i=0}^{d}\theta_i x_i=\theta^Tx$ where $x_0=1$</li> <li>The $L^2$ Loss function is  $\mathbb{L}(y_i,\hat{y_i})=(y_i-\hat{y_i})^2$</li> <li>if we take the sum of all the loss functions over the training set, we get the cost function</li> <li>$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}L(y_i,\hat{y_i})=\frac{1}{2}\sum_{i=1}^{m} L(y_i,h_{\theta}(x_i))=\frac{1}{2}\sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2$</li> </ul> <p>Now this is Cost Function, we are looking for the values which minimize this function.</p> </blockquote> <p>Detail to ponder over: Here the half($\frac{1}{2}$) in cost function is added to simplify the calculation and nothing else. Any positive monotonic transformation of the objective function won’t change the points of minima($arg \hspace{2 mm}min$).</p> <blockquote> <p>Gradient Descent Algorithm</p> <ul> <li>Update rule $\theta_j \leftarrow \theta_j +\Delta \theta_j$ with an update $\Delta \theta_j =-\alpha \frac{\partial J(\theta)}{\partial \theta_j}$, element wise for $d=1,…,m$</li> </ul> <p>for the above $L^2$ Loss function, upon working out the derivative, &gt;we get it as the following, \(\frac{\partial J(\theta)}{\partial \theta}= \sum_{i=1}^{m} (\theta^T x^{(i)}-y^{(i)})x^{(i)}\)</p> <p>Therefore, the update will become, \(\theta:= \theta + \alpha \sum_{i=1}^m (y^{i}-\theta^T x^{i})x^{i}\)</p> <p>To sum it up in vector notation, it will be(for a generic cost &gt;function $J(\theta)$) \(\theta :=\theta -\alpha \nabla_\theta J(\theta)\)</p> </blockquote> <p>Here is the code implementation of it, I’ll briefly discuss the idea of it, so basically we are building a class which will take in the training data and</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <p>Thats all the library imports we’ll be using</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">training_data</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        training_data: The data that is to be considered for the supervised learning problem </span><span class="se">\n</span><span class="s">
        start: Initial guess of the parameters </span><span class="se">\n</span><span class="s">
        learning_rate: Rate which determines the step size for every iteration </span><span class="se">\n</span><span class="s">
        num_iterations: Total Number of Iterations
        </span><span class="sh">"""</span>
        <span class="c1"># Convert training data to matrices
</span>        <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x_vec</span><span class="p">,</span> <span class="n">y_val</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
            <span class="n">X</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="n">x_vec</span><span class="p">))</span>  <span class="c1"># Add bias term
</span>            <span class="n">y</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Shape: (m, n+1) where m=samples, n=features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Shape: (m,)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span> <span class="c1"># number of training examples
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">start</span><span class="p">)</span> <span class="c1"># Inital starting point
</span>        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>
        <span class="n">self</span><span class="p">.</span><span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tuple</span><span class="p">(</span><span class="n">start</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">compute_gradient_loop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">current</span><span class="p">):</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
            <span class="n">x_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># i-th training example
</span>            <span class="n">y_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># i-th target
</span>            <span class="n">residual</span> <span class="o">=</span> <span class="n">y_i</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">x_i</span><span class="p">)</span>
            <span class="n">grad</span> <span class="o">+=</span> <span class="n">residual</span> <span class="o">*</span> <span class="n">x_i</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">current</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">):</span>
            <span class="n">grad</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_gradient_loop</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">current</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
            <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">current</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">query_point</span><span class="p">):</span>
        <span class="n">query_point_with_dummy</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">query_point</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">query_point_with_dummy</span><span class="p">))</span>

</code></pre></div></div> <h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2> <p>Now lets build another class which will run the stochastic gradient descent, as the name suggest, it is stochastic- the parameter value update is now based on the gradient of a single training example(choosen randomly from the training set) and this greatly reduces the run-time(as now you wont be using the all the training samples to calculate the gradient).</p> <p>I’ll show the results. Particularly when the training set is large, the Stochastic Gradient Descent is preferred over the Batch Gradient Descent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">class</span> <span class="nc">StochasticGradientDescent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">training_data</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        training_data: The data that is to be considered for the supervised learning problem </span><span class="se">\n</span><span class="s">
        start: Initial guess of the parameters </span><span class="se">\n</span><span class="s">
        learning_rate: Rate which determines the step size for every iteration </span><span class="se">\n</span><span class="s">
        num_iterations: Total Number of Iterations
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="o">=</span><span class="p">[(</span><span class="nf">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="o">=</span><span class="n">start</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="o">=</span><span class="n">num_iterations</span>
        <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="o">=</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">point</span><span class="p">,</span><span class="n">i</span><span class="p">):</span>
        <span class="n">grad</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
        <span class="n">point</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
        <span class="n">x_i</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y_i</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">residual</span><span class="o">=</span><span class="n">y_i</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">point</span><span class="p">,</span><span class="n">x_i</span><span class="p">)</span>
        <span class="n">grad</span><span class="o">+=</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span><span class="o">*</span><span class="n">x_i</span>
        <span class="k">return</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">):</span>
            <span class="n">i</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">))</span>
            <span class="n">current</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">gradient</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_gradient</span><span class="p">(</span><span class="n">current</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
            <span class="n">update</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">current</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">current</span><span class="p">)))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">query_point</span><span class="p">):</span>
        <span class="n">query_point_with_dummy</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">query_point</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">query_point_with_dummy</span><span class="p">))</span>
</code></pre></div></div> <p>Now let’s put these two algorithms to a race,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#importing the time module to compare the run-time
</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="p">[([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="mf">12.0</span><span class="p">),([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">6.0</span><span class="p">],</span> <span class="mf">19.0</span><span class="p">),</span> <span class="p">([</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="mf">15.0</span><span class="p">)]</span>

<span class="n">gd</span> <span class="o">=</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">10e5</span><span class="p">),</span> <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">)</span>
<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">gd</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">parameters from Batch GD: </span><span class="si">{</span><span class="n">gd</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">time taken: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">prediction for (3,3,3,3): </span><span class="si">{</span><span class="n">gd</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="n">sgd</span> <span class="o">=</span> <span class="nc">StochasticGradientDescent</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">10e5</span><span class="p">),</span> <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">)</span>
<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">sgd</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">parameters from Stochastic GD: </span><span class="si">{</span><span class="n">sgd</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">time taken: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">prediction for (3,3,3,3): </span><span class="si">{</span><span class="n">sgd</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Let’s take a look at the output,</p> <p><em>Red Alert: I have a pretty normal Laptop</em></p> <pre><code class="language-output">parameters from Batch GD: (0.2341891045710697, 1.0012523481527535, 0.9993738259236024, 0.9367564182842788, 1.003757044458375)

time taken: 28.767800092697144

prediction for (3,3,3,3): 12.057608015028098

parameters from Stochastic GD: (0.23418910457018247, 1.001252348153782, 0.9993738259231028, 0.9367564182807299, 1.003757044461383)

time taken: 25.743896484375

prediction for (3,3,3,3): 12.057608015027176
</code></pre> <p>We can see that for a mere $1,00,000$ iterations, we get the parameter estimates from both the algorithms to be very very close. Another point worth noting is the fact that, the Stochastic Gradient Descent was faster than Gradient Decent by more than 4 seconds on my laptop. This could be exploited to a great extent when the training set is huge.</p> <p>Literature also has the mention of Stochastic Batch Gradient Descent(SBGD), which is a slight ugrade over the vanilla SGD as here in SBGD, we sample n(Batch Size) without replacement and go about following the same steps of SGD. SBGD is proven to yield us better results but just a tad bit noiser in the training.</p> <hr/> <h3 id="conclusion">Conclusion</h3> <p>In summary, these two algorithms are commonly used to training large neural nets(or, some convex combination of them). The underlying mathematical foundations underlying these methods are truly elegant!</p> <p>If you are interested in experimenting with the code, you can access the accompanying Python notebook for hands-on practice.</p> <p>Happy Learning!</p>]]></content><author><name></name></author><category term="From Scratch"/><summary type="html"><![CDATA[Hell yeah, Here we go again! It’s been some time, that’s for sure.]]></summary></entry><entry><title type="html">Stack and Queue Implementation</title><link href="https://gouthamsaipydi.github.io/blog/2025/Stack_and_Queue/" rel="alternate" type="text/html" title="Stack and Queue Implementation"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/Stack_and_Queue</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/Stack_and_Queue/"><![CDATA[<p>Here I plan to document the Stack and Queue Implementations in Python, these could be helpful to anyone preparing for the upcoming placement season.</p> <h2 id="stack">Stack</h2> <p>Simply put it’s a linear data structure(linear data container) which holds data with a few quirky twist: 1) Data can only be inserted from one end 2) Inserted data can be removed from that very en</p> <p>I know I made it a bit wordy, the famous 4 letter acronymn LIFO(Last-in First-out) principle nicely sums up the above jargon.</p> <p>Now Let’s implement it in Python to see its beauty.</p> <pre><code class="language-Python">class stack:
    def __init__(self,size):
        if size&lt;=0 or type(size)==float:
            raise ValueError('Stack size must be a positive integer')
        self.size=size
        self.top=-1
        self.arr=[]

    def push(self,x):
        if self.top&gt;=self.size-1:
            raise OverflowError('Stack is full')
        self.arr.append(x)
        self.top+=1
    
    def pop(self):
        if self.top==-1:
            raise IndexError('The Stack is empty, cannot pop')
        self.top-=1
        self.arr.pop()


    def peek(self):
        if self.top==-1:
            raise IndexError('The Stack is empty')
        return self.arr[self.top]

    def length(self):
        return self.top+1
</code></pre> <h3 id="the-important-methods-involved-with-stack-data-structure-are">The Important Methods involved with Stack Data Structure are</h3> <p>push, pop, peek and length.</p> <p>Push Method: As the word suggests, it pushes elements on to the stack from one end.</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Pop Method: It pops the last element which was pushed onto the stack(LIFO is the Magic Mantra)</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Peek Method: This method enables us to take a quick peek at the stack.</p> <ul> <li>Time Complexity involed is O(1)</li> </ul> <p>Length Method: This method enables us to get the size of the stack.</p> <h2 id="queue">Queue</h2> <p>It’s also a linear data structure which holds data in the following way: 1) Data can only be inserted from one end 2) Inserted data can be removed from the opposite end</p> <p>The 4 letter acronymn which can come in handy is FIFO(First-in First-out)</p> <p>Now Let’s implement it in Python to better understand this data structure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Queue</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">size</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span><span class="o">*</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="n">self</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nc">OverflowError</span><span class="p">(</span><span class="sh">"</span><span class="s">Queue is full</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="n">self</span><span class="p">.</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="p">]</span><span class="o">=</span><span class="n">x</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">+=</span><span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">IndexError</span><span class="p">(</span><span class="sh">"</span><span class="s">Queue is empty</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">a</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">end</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="n">self</span><span class="p">.</span><span class="n">size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">-=</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">a</span>

    <span class="k">def</span> <span class="nf">peek</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nc">IndexError</span><span class="p">(</span><span class="sh">"</span><span class="s">Queue is Empty</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">arr</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">length</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">current_size</span>
</code></pre></div></div> <h3 id="the-important-methods-involved-with-queue-data-structure-are">The Important Methods involved with Queue Data Structure are</h3> <p>push, pop, peek and length.</p> <p>Push Method: It pushes elements on to the queue from one end.</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Pop Method: It pops the last element which was pushed first onto the queue</p> <ul> <li>Time Complexity involved is O(1)</li> </ul> <p>Peek Method: This method enables us to take a quick peek at the queue</p> <ul> <li>Time Complexity involed is O(1)</li> </ul> <p>Length Method: This method enables us to get the size of the queue.</p> <ul> <li>Time Complexity involed is O(1)</li> </ul>]]></content><author><name></name></author><category term="DSA"/><category term="python"/><summary type="html"><![CDATA[Here I plan to document the Stack and Queue Implementations in Python, these could be helpful to anyone preparing for the upcoming placement season.]]></summary></entry><entry><title type="html">Arguably the Most Handy Python Library- Sympy</title><link href="https://gouthamsaipydi.github.io/blog/2025/Sympy/" rel="alternate" type="text/html" title="Arguably the Most Handy Python Library- Sympy"/><published>2025-01-20T00:00:00+00:00</published><updated>2025-01-20T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2025/Sympy</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2025/Sympy/"><![CDATA[<h2 id="sympy--the-library-hiding-under-the-hood">Sympy- The Library hiding under the hood</h2> <p>For the longest time, I had no clue this library was out there, and when I finally found it,It just blew my mind, I couldn’t stop thinking, “Why didn’t I know about this sooner?” But its just no use crying over spilled milk. Now let’s get straight to the good stuff and explore the magic of Sympy.</p> <p><a href="https://www.sympy.org/en/index.html">Sympy</a> stands for Symbolic Python. It was developed 17 years ago. Be it ODE’s or algebraic equations or solving the diophantine equations algebraically, look no further-Sympy has got you covered.</p> <h3 id="whats-so-special-about-this">What’s so special about this?</h3> <p>As most of you might be aware of the python library <a href="https://numpy.org/">Numpy</a>, which enables you to perform numerical calculations on your data. But Sympy is a bit more abstract in that sense, it enables you to perform symbolic computation(Using symbols)</p> <p>Simply put, we can perform all those nightmare inducing, pain staking and laborious mathematical operations like solving ODEs,substitution, differentiation, integration and even writing the equation in LaTex using sympy.</p> <p>Let me show you the beauty of it. It goes without saying that to use the library you need to first pip it :-}</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sympy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">f,x</span><span class="sh">'</span><span class="p">)</span>

<span class="c1">#Lets get creative about the function 
</span><span class="n">f</span><span class="o">=</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>

<span class="c1">#Interested in the functional value of the above function at x=2? Fret not dear
</span><span class="n">f</span><span class="p">.</span><span class="nf">subs</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span><span class="mi">2</span><span class="p">)])</span>
<span class="c1"># Output 54.5981500331442
</span>
<span class="c1"># Lets differentiate this Monster 
</span><span class="n">f</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># 1.5*x**2*exp(x**3)**0.5
</span></code></pre></div></div> <p>That’s just a short premise into how useful this library can be.</p> <p>Here is another snippet which performs the following daunting mathematical operation easily</p> \[\int_0^{3\pi}x^{12}*sin(33x)\hspace{1mm}dx\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">f</span><span class="sh">'</span><span class="p">,</span><span class="n">cls</span><span class="o">=</span><span class="n">Function</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">)</span>
<span class="c1">#defining the function
</span><span class="n">f</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">12</span><span class="o">*</span><span class="nf">sin</span><span class="p">(</span><span class="mi">33</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="n">f</span><span class="p">.</span><span class="nf">integrate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1">#Output: -x**12*cos(33*x)/33 + 4*x**11*sin(33*x)/363 + 4*x**10*cos(33*x)/1089 - 40*x**9*sin(33*x)/35937 - 40*x**8*cos(33*x)/131769 + 320*x**7*sin(33*x)/4348377 + 2240*x**6*cos(33*x)/143496441 - 4480*x**5*sin(33*x)/1578460851 - 22400*x**4*cos(33*x)/52089208083 + 89600*x**3*sin(33*x)/1718943866739 + 89600*x**2*cos(33*x)/18908382534129 - 179200*x*sin(33*x)/623976623626257 - 179200*cos(33*x)/20591228579666481 
</span>
<span class="n">integral</span><span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="nf">integrate</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">subs</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">pi</span><span class="p">)])</span><span class="o">-</span><span class="n">f</span><span class="p">.</span><span class="nf">integrate</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">subs</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span><span class="mi">0</span><span class="p">)]))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">integral</span><span class="p">)</span>
<span class="c1"># Output: 14864390294.783045
</span></code></pre></div></div> <p>Another cool method which i found to be very useful was print_latex, I’ll use that method trying to solve the following ODE.</p> \[12*y^{\prime \prime }(x)+3y^{\prime}(x)-6*y(x)=0\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span><span class="o">=</span><span class="nf">symbols</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span><span class="n">cls</span><span class="o">=</span><span class="n">Function</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">dsolve</span><span class="p">(</span><span class="mi">12</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">6</span><span class="o">*</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="c1"># Output:
# Eq(y(x), C1*exp(x*(-1 + sqrt(33))/8) + C2*exp(-x*(1 + sqrt(33))/8))
</span><span class="nf">checkodesol</span><span class="p">(</span><span class="mi">12</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="nc">Derivative</span><span class="p">(</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">6</span><span class="o">*</span><span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: (True, 0)
</span><span class="nf">print_latex</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># Output: y{\left(x \right)} = C_{1} e^{\frac{x \left(-1 + \sqrt{33}\right)}{8}} + C_{2} e^{- \frac{x \left(1 + \sqrt{33}\right)}{8}}
</span></code></pre></div></div> <p>Wondering what the solution, Lo and Behold here it is, \(y{\left(x \right)} = C_{1} e^{\frac{x \left(-1 + \sqrt{33}\right)}{8}} + C_{2} e^{- \frac{x \left(1 + \sqrt{33}\right)}{8}}\)</p> <p>Well here i have only mentioned a few instances where sympy can come in handy. Gloss over the <a href="https://docs.sympy.org/latest/index.html">Documentation</a>, you will find many more interesting applications.</p> <p>Happy Learning!</p>]]></content><author><name></name></author><category term="python"/><summary type="html"><![CDATA[Sympy- The Library hiding under the hood]]></summary></entry><entry><title type="html">What If?</title><link href="https://gouthamsaipydi.github.io/blog/2024/First_Blog_Post/" rel="alternate" type="text/html" title="What If?"/><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://gouthamsaipydi.github.io/blog/2024/First_Blog_Post</id><content type="html" xml:base="https://gouthamsaipydi.github.io/blog/2024/First_Blog_Post/"><![CDATA[<p>Welcome to my first blog post!</p> <p>I have always wondered how would it feel to able to express yourself without a care in the world. It’s this overarching curiosity that’s driving me to pen this down. I’m planning of documenting my views on the books and articles that i go through.</p> <p>Let me be clear with something, 1) English is not my first language, and 2) I am an amateur writer, don’t expect Shakespearean from me.</p> <p>Brace yourselves to experience an opionated mind!</p> <p>Prost</p>]]></content><author><name></name></author><category term="Introduction"/><summary type="html"><![CDATA[Hello World]]></summary></entry></feed>