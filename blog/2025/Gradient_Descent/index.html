<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gradient Descent from Scratch | Goutham Sai Pydi </title> <meta name="author" content="Goutham Sai Pydi"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gouthamsaipydi.github.io/blog/2025/Gradient_Descent/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Goutham</span> Sai Pydi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gradient Descent from Scratch</h1> <p class="post-meta"> Created in July 31, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/from-scratch"> <i class="fa-solid fa-tag fa-sm"></i> From Scratch</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hell yeah, Here we go again! It’s been some time, that’s for sure.</p> <p>I’m thinking of starting a new series of blog posts where we’ll be trying to implement things from scratch; Let’s kick start things with gradient descent algorithm(from scratch) with only using NumPy(Numerical Python Library). I’ll be writing the code alongside the theory to better understand this super awesome algorithm.</p> <p>There are plenty of guides over the internet on this, you can name it protege effect or anything, but teaching others only enhances your own clarity on the topic. Okay lets cut to the chase and jump right into the topic.</p> <h2 id="introduction">Introduction</h2> <p>With the onset of big data- Neural nets have found there way into the mainstream. To train these mammoth neural nets, we are always looking for ways to save our expenses while not compromising on the quality of the training.</p> <h3 id="whats-our-approach">What’s our approach?</h3> <p>We are trying to minimize a “decided” Loss function(with loads of parameters, like loads and loads), i.e., looking out for suitable values for those parameters which minimize the above decided loss function.</p> <p>Note: I’m using <em>decided</em>, as the loss depends on the problem we are trying to solve.</p> <p>More mathematically, we have an optimization problem as, \(\min_{w\in\mathbb{R}^D}E(w)\) \(w^*=\text{arg} \min_{w\in\mathbb{R}^D}E(w)\)</p> <p>$w^*$ is the vector, why we are doing all this. It will be our gateway to awesome predictions.</p> <p>The Objective function [$E(w)$ above] is usually a super complicated function; by complicated, i mean some function which cannot be solved in closed form($w^*$= some function involving the independent variables), and needs some deep introspection to find its global minima.</p> <p>There are plenty of iterative methods to solve for the above optimisation problems. Like the Newton Method and all, but the one which has took the world by storm is- Gradient Descent and Stochastic Gradient descent, a clever extension which serves as the basis for training super large neural nets.</p> <h2 id="gradient-descent">Gradient Descent</h2> <p>It’s basically an Algorithm, if followed with the right-set of hyperparamters will fetch you the golden $w^*$ that you are looking out for.</p> <p>Getting into math of it, its an iterative first-order algorithm for minimizing a differentiable multivariate function.</p> <p>Let’s get a few mechanical details out of the way;</p> <p>Gradient Descent $\iff$ used for finding the local minima of multivariate functions</p> <p>Gradient Ascent $\iff$ used for finding the local maxima of multivariate functions</p> <p>Let me walk you through a short lecture on gradients,</p> <table> <tbody> <tr> <td>For a Multi-Variable function $F(x)$ defined and differentiable in a neighborhood of a point $a$, then $F(x)$ increases the fastest if one goes from $a$ in the direction of the gradient of $F$ at $a$ i.e., $\frac{\partial F}{\partial x}{\Large</td> <td>}_{x=a}$</td> </tr> </tbody> </table> <p>In simple English, a recursive operation to look for the local minimum of seemingly daunting functions.</p> <p>Idea is that we use iterative methods to solve the optimization problem, \(w^{(0)}\rightarrow w^{(1)}... \rightarrow w^{(\infty)}=w^\star\)</p> <p>We’ll be initializing the weight vector $w^{(0)}$ usually with small random numbers; I’m using the zero index to signify that as our initial guess of optimal parameters. We’ll continue using our recursive function to get hold of the minimum.</p> <h3 id="when-to-stop">When to stop?</h3> <p>This is a million-dollar question and the stopping criteria can vary- It can be limited to a set number of iterations. The difference between the updates is quite small and many others.</p> <p>Which one to choose is simply upto the user and the problem that their trying to solve.</p> <p>Let’s jump back to the algorithm,</p> <p><em>Notation Alert: The number of training samples ${(x^{(i)},y^{(i)})}$ is $m$</em></p> <blockquote> <p><strong>Example</strong> Eg: $w_d^{(0)}\sim$ uniform$[-0.01,0.01]$ : $\forall d$ Initial weight vector $w^{(0)}$: usually small random numbers.</p> <p>For Demonstration purpose, lets assume the squared loss function (the el classico of all loss functions), and later I will work out the details for $L^1$ Loss function as well.</p> <ul> <li>We are working with a supervised learning algorithm, so there will be a training set of the form,<br>    $T={(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$<br>    where each $x^{(i)}$ is a $p$ dimensional vector $: \forall i$</li> <li>Consider the ordinary least square regression model, then the hypothesis function is: $h_{\theta}(x)=\sum_{i=0}^{d}\theta_i x_i=\theta^Tx$ where $x_0=1$</li> <li>The $L^2$ Loss function is  $\mathbb{L}(y_i,\hat{y_i})=(y_i-\hat{y_i})^2$</li> <li>if we take the sum of all the loss functions over the training set, we get the cost function</li> <li>$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}L(y_i,\hat{y_i})=\frac{1}{2}\sum_{i=1}^{m} L(y_i,h_{\theta}(x_i))=\frac{1}{2}\sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2$</li> </ul> <p>Now this is Cost Function, we are looking for the values which minimize this function.</p> </blockquote> <p>Detail to ponder over: Here the half($\frac{1}{2}$) in cost function is added to simplify the calculation and nothing else. Any positive monotonic transformation of the objective function won’t change the points of minima($arg \hspace{2 mm}min$).</p> <blockquote> <p>Gradient Descent Algorithm</p> <ul> <li>Update rule $\theta_j \leftarrow \theta_j +\Delta \theta_j$ with an update $\Delta \theta_j =-\alpha \frac{\partial J(\theta)}{\partial \theta_j}$, element wise for $d=1,…,m$</li> </ul> <p>for the above $L^2$ Loss function, upon working out the derivative, &gt;we get it as the following, \(\frac{\partial J(\theta)}{\partial \theta}= \sum_{i=1}^{m} (\theta^T x^{(i)}-y^{(i)})x^{(i)}\)</p> <p>Therefore, the update will become, \(\theta:= \theta + \alpha \sum_{i=1}^m (y^{i}-\theta^T x^{i})x^{i}\)</p> <p>To sum it up in vector notation, it will be(for a generic cost &gt;function $J(\theta)$) \(\theta :=\theta -\alpha \nabla_\theta J(\theta)\)</p> </blockquote> <p>Here is the code implementation of it, I’ll briefly discuss the idea of it, so basically we are building a class which will take in the training data and</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div> <p>Thats all the library imports we’ll be using</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GradientDescent</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">training_data</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        training_data: The data that is to be considered for the supervised learning problem </span><span class="se">\n</span><span class="s">
        start: Initial guess of the parameters </span><span class="se">\n</span><span class="s">
        learning_rate: Rate which determines the step size for every iteration </span><span class="se">\n</span><span class="s">
        num_iterations: Total Number of Iterations
        </span><span class="sh">"""</span>
        <span class="c1"># Convert training data to matrices
</span>        <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x_vec</span><span class="p">,</span> <span class="n">y_val</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
            <span class="n">X</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="n">x_vec</span><span class="p">))</span>  <span class="c1"># Add bias term
</span>            <span class="n">y</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Shape: (m, n+1) where m=samples, n=features
</span>        <span class="n">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Shape: (m,)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span> <span class="c1"># number of training examples
</span>        <span class="n">self</span><span class="p">.</span><span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">start</span><span class="p">)</span> <span class="c1"># Inital starting point
</span>        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">=</span> <span class="n">num_iterations</span>
        <span class="n">self</span><span class="p">.</span><span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="nf">tuple</span><span class="p">(</span><span class="n">start</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">compute_gradient_loop</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">current</span><span class="p">):</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
            <span class="n">x_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># i-th training example
</span>            <span class="n">y_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># i-th target
</span>            <span class="n">residual</span> <span class="o">=</span> <span class="n">y_i</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">x_i</span><span class="p">)</span>
            <span class="n">grad</span> <span class="o">+=</span> <span class="n">residual</span> <span class="o">*</span> <span class="n">x_i</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">current</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">):</span>
            <span class="n">grad</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_gradient_loop</span><span class="p">(</span><span class="n">current</span><span class="p">)</span>
            <span class="n">current</span> <span class="o">=</span> <span class="n">current</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
            <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">current</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">query_point</span><span class="p">):</span>
        <span class="n">query_point_with_dummy</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">query_point</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">query_point_with_dummy</span><span class="p">))</span>

</code></pre></div></div> <h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2> <p>Now lets build another class which will run the stochastic gradient descent, as the name suggest, it is stochastic- the parameter value update is now based on the gradient of a single training example(choosen randomly from the training set) and this greatly reduces the run-time(as now you wont be using the all the training samples to calculate the gradient).</p> <p>I’ll show the results. Particularly when the training set is large, the Stochastic Gradient Descent is preferred over the Batch Gradient Descent.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 

<span class="k">class</span> <span class="nc">StochasticGradientDescent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">training_data</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        training_data: The data that is to be considered for the supervised learning problem </span><span class="se">\n</span><span class="s">
        start: Initial guess of the parameters </span><span class="se">\n</span><span class="s">
        learning_rate: Rate which determines the step size for every iteration </span><span class="se">\n</span><span class="s">
        num_iterations: Total Number of Iterations
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="o">=</span><span class="p">[(</span><span class="nf">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">start</span><span class="o">=</span><span class="n">start</span>
        <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="o">=</span><span class="n">num_iterations</span>
        <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="o">=</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">point</span><span class="p">,</span><span class="n">i</span><span class="p">):</span>
        <span class="n">grad</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
        <span class="n">point</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>
        <span class="n">x_i</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y_i</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">residual</span><span class="o">=</span><span class="n">y_i</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">point</span><span class="p">,</span><span class="n">x_i</span><span class="p">)</span>
        <span class="n">grad</span><span class="o">+=</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span><span class="o">*</span><span class="n">x_i</span>
        <span class="k">return</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">):</span>
            <span class="n">i</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">))</span>
            <span class="n">current</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">gradient</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_gradient</span><span class="p">(</span><span class="n">current</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
            <span class="n">update</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">current</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">current</span><span class="p">)))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">query_point</span><span class="p">):</span>
        <span class="n">query_point_with_dummy</span><span class="o">=</span><span class="p">[</span><span class="mf">1.0</span><span class="p">]</span><span class="o">+</span><span class="nf">list</span><span class="p">(</span><span class="n">query_point</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">query_point_with_dummy</span><span class="p">))</span>
</code></pre></div></div> <p>Now let’s put these two algorithms to a race,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#importing the time module to compare the run-time
</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="p">[([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="mf">12.0</span><span class="p">),([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span><span class="mf">6.0</span><span class="p">],</span> <span class="mf">19.0</span><span class="p">),</span> <span class="p">([</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span><span class="mf">5.0</span><span class="p">],</span> <span class="mf">15.0</span><span class="p">)]</span>

<span class="n">gd</span> <span class="o">=</span> <span class="nc">GradientDescent</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">10e5</span><span class="p">),</span> <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">)</span>
<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">gd</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">parameters from Batch GD: </span><span class="si">{</span><span class="n">gd</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">time taken: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">prediction for (3,3,3,3): </span><span class="si">{</span><span class="n">gd</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

<span class="n">sgd</span> <span class="o">=</span> <span class="nc">StochasticGradientDescent</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">10e5</span><span class="p">),</span> <span class="n">training_data</span><span class="o">=</span><span class="n">training_data</span><span class="p">)</span>
<span class="n">start</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">sgd</span><span class="p">.</span><span class="nf">run</span><span class="p">()</span>
<span class="n">end</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">parameters from Stochastic GD: </span><span class="si">{</span><span class="n">sgd</span><span class="p">.</span><span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">time taken: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">prediction for (3,3,3,3): </span><span class="si">{</span><span class="n">sgd</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Let’s take a look at the output,</p> <p><em>Red Alert: I have a pretty normal Laptop</em></p> <pre><code class="language-output">parameters from Batch GD: (0.2341891045710697, 1.0012523481527535, 0.9993738259236024, 0.9367564182842788, 1.003757044458375)

time taken: 28.767800092697144

prediction for (3,3,3,3): 12.057608015028098

parameters from Stochastic GD: (0.23418910457018247, 1.001252348153782, 0.9993738259231028, 0.9367564182807299, 1.003757044461383)

time taken: 25.743896484375

prediction for (3,3,3,3): 12.057608015027176
</code></pre> <p>We can see that for a mere $1,00,000$ iterations, we get the parameter estimates from both the algorithms to be very very close. Another point worth noting is the fact that, the Stochastic Gradient Descent was faster than Gradient Decent by more than 4 seconds on my laptop. This could be exploited to a great extent when the training set is huge.</p> <p>Literature also has the mention of Stochastic Batch Gradient Descent(SBGD), which is a slight ugrade over the vanilla SGD as here in SBGD, we sample n(Batch Size) without replacement and go about following the same steps of SGD. SBGD is proven to yield us better results but just a tad bit noiser in the training.</p> <hr> <h3 id="conclusion">Conclusion</h3> <p>In summary, these two algorithms are commonly used to training large neural nets(or, some convex combination of them). The underlying mathematical foundations underlying these methods are truly elegant!</p> <p>If you are interested in experimenting with the code, you can access the accompanying Python notebook for hands-on practice.</p> <p>Happy Learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/7-Pandas-Methods-everybody-should-know/">7 Pandas Methods everybody should know</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/7_Pandas_Methods_everybody_should_know/">7 Pandas Methods everybody should know</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Stack_and_Queue/">Stack and Queue Implementation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Sympy/">Arguably the Most Handy Python Library- Sympy</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Sapiens/">Homo Sapien- A Two legged menace</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Goutham Sai Pydi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"Repositories",description:"Welcome to my space, where I\u2019m working on inventing some cool stuff.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-7-pandas-methods-everybody-should-know",title:"7 Pandas Methods everybody should know",description:"A short compilation of the most used pandas methods",section:"Posts",handler:()=>{window.location.href="/blog/2025/7-Pandas-Methods-everybody-should-know/"}},{id:"post-7-pandas-methods-everybody-should-know",title:"7 Pandas Methods everybody should know",description:"A short compilation of the most used pandas methods",section:"Posts",handler:()=>{window.location.href="/blog/2025/7_Pandas_Methods_everybody_should_know/"}},{id:"post-gradient-descent-from-scratch",title:"Gradient Descent from Scratch",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/Gradient_Descent/"}},{id:"post-stack-and-queue-implementation",title:"Stack and Queue Implementation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/Stack_and_Queue/"}},{id:"post-arguably-the-most-handy-python-library-sympy",title:"Arguably the Most Handy Python Library- Sympy",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/Sympy/"}},{id:"post-homo-sapien-a-two-legged-menace",title:"Homo Sapien- A Two legged menace",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Sapiens/"}},{id:"post-what-if",title:"What If?",description:"Hello World",section:"Posts",handler:()=>{window.location.href="/blog/2024/First_Blog_Post/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%61%69%67%6F%75%74%68%61%6D%70%79%64%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/gouthamsaipydi","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/saigouthampydi","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/saigouthampydi","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>